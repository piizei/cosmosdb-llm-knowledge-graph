{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Generation with LLMs\n",
    "![title](neo4jdogs.png)\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to create a knowledge graph from PDF documents using Large Language Models (specifically Azure OpenAI). The process includes:\n",
    "\n",
    "1. Loading and preprocessing PDF documents\n",
    "2. Using LLMs to extract entities and relationships\n",
    "3. Constructing and storing a knowledge graph\n",
    "\n",
    "The resulting graph is serialized for use in companion notebooks:\n",
    "- [Knowledge graph with Neo4J (Cypher)](./graph-neo4j.ipynb)\n",
    "- [Knowledge graph with Azure CosmosDB (Gremlin)](./graph-cosmosdb.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load env variables and connect to Azure Openai\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from logging import StreamHandler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Validate required environment variables\n",
    "required_vars = [\"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_DEPLOYMENT_NAME\", \"AZURE_OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    error_msg = f\"Error: Missing required environment variables: {', '.join(missing_vars)}\"\n",
    "    logger.error(error_msg)\n",
    "    raise EnvironmentError(error_msg)\n",
    "\n",
    "# Configure LLM settings\n",
    "REASONING_EFFORT = \"medium\"  # Options: \"low\", \"medium\", \"high\"\n",
    "MAX_TIMEOUT = 3 * 60 * 1000  # 3 minutes in milliseconds\n",
    "\n",
    "# Initialize LLM client\n",
    "try:\n",
    "    llm = AzureChatOpenAI(\n",
    "        timeout=MAX_TIMEOUT,\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        verbose=True,\n",
    "        reasoning_effort=REASONING_EFFORT,\n",
    "    )\n",
    "    logger.info(f\"Successfully initialized Azure OpenAI client with reasoning effort: {REASONING_EFFORT}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Azure OpenAI client: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "\n",
    "def download_documents(urls: List[str], local_folder: str = \"./data/\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Download documents from URLs and save them to a local folder.\n",
    "    Returns a list of downloaded document names.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of document URLs to download\n",
    "        local_folder: Directory to save downloaded files\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file names\n",
    "    \"\"\"\n",
    "    # Ensure the download directory exists\n",
    "    os.makedirs(local_folder, exist_ok=True)\n",
    "    \n",
    "    doc_names = []\n",
    "    \n",
    "    for url in tqdm(urls, desc=\"Downloading documents\"):\n",
    "        try:\n",
    "            # Extract file name from URL\n",
    "            file_name = url.split(\"/\")[-1]\n",
    "            doc_names.append(file_name)\n",
    "            file_path = os.path.join(local_folder, file_name)\n",
    "            \n",
    "            # Skip if file already exists\n",
    "            if os.path.isfile(file_path):\n",
    "                logger.info(f\"File already exists: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            # Download file\n",
    "            logger.info(f\"Downloading {url}\")\n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "            logger.info(f\"Downloaded {file_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download {url}: {str(e)}\")\n",
    "    \n",
    "    return doc_names\n",
    "\n",
    "# Define document URLs\n",
    "document_urls = [\n",
    "    \"https://www.marinhumane.org/wp-content/uploads/2017/06/Dog-Breed-Characteristics-Behavior.pdf\"\n",
    "    # Add more document URLs here as needed\n",
    "]\n",
    "\n",
    "# Download documents\n",
    "local_folder = \"./data/\"\n",
    "doc_names = download_documents(document_urls, local_folder)\n",
    "logger.info(f\"Downloaded {len(doc_names)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PDF Documents with Langchain\n",
    "Use Langchain's built-in document loaders to process PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "# Create a dictionary to store Document objects for each PDF\n",
    "document_objects = {}\n",
    "all_documents = []\n",
    "\n",
    "# Note that the document-loader is configured to 'single' mode.\n",
    "# This works best if the documents are not too large (over 20 pages or so).\n",
    "\n",
    "for doc_name in doc_names:\n",
    "    print(f\"Processing {doc_name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use PyMuPDFLoader for better PDF text extraction\n",
    "    # If PyMuPDF (fitz) is not installed, you can use PyPDFLoader instead\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(local_folder + doc_name, mode=\"single\")\n",
    "        # Alternative if PyMuPDF is not available:\n",
    "        # loader = PyPDFLoader(local_folder + doc_name)\n",
    "        \n",
    "        # Load the documents - this returns a list of Document objects\n",
    "        documents = loader.load()\n",
    "        document_objects[doc_name] = documents\n",
    "        all_documents.extend(documents)\n",
    "        \n",
    "        # Display information about the loaded documents\n",
    "        print(f\"Loaded {len(documents)} document from {doc_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {doc_name}: {e}\")\n",
    "    \n",
    "    # Calculate processing time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing completed in {elapsed_time:.6f} seconds\\n\")\n",
    "\n",
    "print(f\"Total Document objects created: {len(all_documents)}\")\n",
    "\n",
    "# Display content of the first document to verify extraction\n",
    "if all_documents:\n",
    "    print(\"\\nFirst 200 characters of content:\")\n",
    "    print(all_documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph Extraction\n",
    "\n",
    "Using the LLMGraphTransformer from langchain_experimental to extract entities and relationships from the documents.\n",
    "We'll define allowed node types and configure the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create the transformer with our custom üê∂ configuration\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        allowed_nodes= [\"Breed\", \"BreedingGroup\", \"Characteristics\", \"Trait\", \"Origin\"],\n",
    "        allowed_relationships= [\"HAS_CHARACTERISTIC\", \"BELONGS_TO\", \"ORIGINATED_FROM\"],        \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents_to_graph(documents, batch_size=5):\n",
    "    \"\"\"\n",
    "    Process documents into graph format with progress tracking and error handling.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of Document objects to process\n",
    "        batch_size: Number of documents to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        List of graph documents\n",
    "    \"\"\"\n",
    "    graph_documents = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Processing document batches\"):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        try:\n",
    "            batch_results = llm_transformer.convert_to_graph_documents(batch)\n",
    "            graph_documents.extend(batch_results)\n",
    "            logger.info(f\"Processed batch {i//batch_size + 1} with {len(batch)} documents\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch {i//batch_size + 1}: {str(e)}\")\n",
    "            # Continue with next batch instead of failing completely\n",
    "            continue\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"Processing completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return graph_documents\n",
    "\n",
    "# Process all documents to graph format\n",
    "graph_documents = process_documents_to_graph(all_documents)\n",
    "\n",
    "# Display summary statistics\n",
    "if graph_documents:\n",
    "    total_nodes = sum(len(doc.nodes) for doc in graph_documents)\n",
    "    total_rels = sum(len(doc.relationships) for doc in graph_documents)\n",
    "    print(f\"Extracted {total_nodes} nodes and {total_rels} relationships from {len(graph_documents)} documents\")\n",
    "    \n",
    "    # Display sample from first document\n",
    "    print(\"\\nSample from first document:\")\n",
    "    print(f\"Nodes: {graph_documents[0].nodes[:3]}\" if graph_documents[0].nodes else \"No nodes found\")\n",
    "    print(f\"Relationships: {graph_documents[0].relationships[:3]}\" if graph_documents[0].relationships else \"No relationships found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the graph documents\n",
    "Save the extracted graph data to pickle file for later use (in the other notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle the graph so we don't have to redo this all the time ü•í\n",
    "import pickle\n",
    "\n",
    "with open('./data/graph_docs.pkl','wb') as f:\n",
    "    pickle.dump(graph_documents, f)\n",
    "\n",
    "print(f\"Saved {len(graph_documents)} graph documents to './data/graph_docs.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Analyze the Knowledge Graph\n",
    "\n",
    "Let's create a simple visualization of our knowledge graph using NetworkX and matplotlib.\n",
    "(It looks nicer in Neo4j ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def build_networkx_graph(graph_docs):\n",
    "    \"\"\"\n",
    "    Convert graph documents to a NetworkX graph for visualization and analysis.\n",
    "    \n",
    "    Args:\n",
    "        graph_docs: List of graph documents with nodes and relationships\n",
    "        \n",
    "    Returns:\n",
    "        NetworkX graph object\n",
    "    \"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "    \n",
    "    # Track unique nodes to avoid duplicates\n",
    "    added_nodes = set()\n",
    "    \n",
    "    # Add all nodes first\n",
    "    for doc in graph_docs:\n",
    "        for node in doc.nodes:\n",
    "            node_id = f\"{node.id}\"\n",
    "            if node_id not in added_nodes:\n",
    "                G.add_node(\n",
    "                    node_id,\n",
    "                    label=node.id,\n",
    "                    type=node.type,\n",
    "                    properties=node.properties\n",
    "                )\n",
    "                added_nodes.add(node_id)\n",
    "    \n",
    "    # Then add all relationships\n",
    "    for doc in graph_docs:\n",
    "        for rel in doc.relationships:\n",
    "            G.add_edge(\n",
    "                f\"{rel.source}\",\n",
    "                f\"{rel.target}\",\n",
    "                type=rel.type,\n",
    "                properties=rel.properties\n",
    "            )\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build the graph\n",
    "try:\n",
    "    G = build_networkx_graph(graph_documents)\n",
    "    \n",
    "    # Print graph statistics\n",
    "    print(f\"Graph Statistics:\")\n",
    "    print(f\"- Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"- Edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    # Count node types - safely handling missing 'type' attribute\n",
    "    node_types = [data.get('type', 'Unknown') for _, data in G.nodes(data=True)]\n",
    "    type_counts = Counter(node_types)\n",
    "    print(\"\\nNode type distribution:\")\n",
    "    for node_type, count in type_counts.items():\n",
    "        print(f\"- {node_type}: {count} nodes\")\n",
    "    \n",
    "    # Count relationship types - safely handling missing 'type' attribute\n",
    "    rel_types = [data.get('type', 'Unknown') for _, _, data in G.edges(data=True)]\n",
    "    rel_counts = Counter(rel_types)\n",
    "    print(\"\\nRelationship type distribution:\")\n",
    "    for rel_type, count in rel_counts.items():\n",
    "        print(f\"- {rel_type}: {count} relationships\")\n",
    "    \n",
    "    # Visualize the graph\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Use different colors for different node types\n",
    "    node_colors = []\n",
    "    color_map = {\n",
    "        'Breed': 'skyblue',\n",
    "        'BreedingGroup': 'lightgreen',\n",
    "        'Characteristics': 'salmon',\n",
    "        'Trait': 'orange',\n",
    "        'Origin': 'purple',\n",
    "        'Unknown': 'gray'\n",
    "    }\n",
    "    \n",
    "    for _, node_data in G.nodes(data=True):\n",
    "        node_type = node_data.get('type', 'Unknown')\n",
    "        node_colors.append(color_map.get(node_type, 'gray'))\n",
    "    \n",
    "    # Position nodes using spring layout\n",
    "    pos = nx.spring_layout(G, seed=42)  # For reproducibility\n",
    "    \n",
    "    # Draw nodes and edges\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=700, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, arrowsize=20)\n",
    "    \n",
    "    # Add labels with smaller font size\n",
    "    labels = {node: G.nodes[node].get('label', node) for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    # Add a legend\n",
    "    legend_patches = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                label=node_type,\n",
    "                                markerfacecolor=color, markersize=10)\n",
    "                      for node_type, color in color_map.items() if node_type in type_counts]\n",
    "    plt.legend(handles=legend_patches, loc='upper right')\n",
    "    \n",
    "    plt.title('Knowledge Graph Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./data/knowledge_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error visualizing graph: {str(e)}\")\n",
    "    # Print more detailed error information for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
