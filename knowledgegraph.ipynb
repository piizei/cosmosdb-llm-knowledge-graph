{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create knowledge graph from pdf with LLM.\n",
    "![title](neo4jdogs.png)\n",
    "\n",
    "This notebook creates a Knowledge graph with help of OpenAI GPT4. It's stored into a pickle-file, and you can load and use it in following notebooks:\n",
    "- [Knowledge graph with Neo4J (Cypher)](./graph-neo4j.ipynb)\n",
    "- [Knowledge graph with Azure CosmosDB (Gremlin)](./graph-cosmosdb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load env variables and connect to neo4j database\n",
    "Please run _docker-compose up_ first on the directory to start the database.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import OutputParserException\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=os.getenv(\"OPENAI_DEPLOYMENT_NAME\"), \n",
    "    temperature=0, \n",
    "    max_tokens=4000,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Pydantic model of the graph.\n",
    "# The Langchain KnowledgeGraph model is too complicated as OpenAI functions schema \n",
    "\n",
    "\n",
    "from typing import List, Dict, Optional, Union\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseModel):\n",
    "    \"Represents a node in a graph with associated properties\"\n",
    "    id: Union[str, int]\n",
    "    type: Optional[str] = \"Node\"\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"Represents a directed relationship between two nodes in a graph.\"\n",
    "    source: Union[str, int] = Field(..., description=\"Id of source node\")\n",
    "    target: Union[str, int] = Field(..., description=\"Id of target node\")\n",
    "    type: Optional[str] =  Field(..., description=\"Type of relationship\")\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "    \n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Knowlege graph consisting of nodes and relationships\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic\n",
    "The prompt, the function call and the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.tracers import ConsoleCallbackHandler\n",
    "from langchain.tools import tool\n",
    "\n",
    "# The prompt is from langchain examples\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "*Double check* that the JSON structure is correct.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Store function call results to this list\n",
    "function_responses = []\n",
    "\n",
    "# Define function call in form of a langchain tool (it's converted to a open-ai function). The function schema is defined with pydantic.\n",
    "@tool\n",
    "def knowledge_graph(object: KnowledgeGraph) -> Dict[str, List]:\n",
    "    \"\"\"A Tool to convert text to knowledge grap\"\"\"\n",
    "    function_responses.append(object)\n",
    "    return  True # Don't return anything for the LLM so that the context does not grow.\n",
    "\n",
    "\n",
    "# Added some more precise instructions to have more control over the output. \n",
    "# Likely you would have existing schemas or terminology that you would like to reuse.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\",system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "                (\"human\", \n",
    "                 \"\"\"In this particular case we are interested in dogs. We want to extract information about dog breeds and their characteristics.\n",
    "                    Characterics should be nodes and relationships should be between dog breeds and their characteristics. \n",
    "                    Ignore other entities than dogs, like people and addresses.\n",
    "                    - **Allowed Node Labels:** Breed, BreedingGroup, Characteristic\n",
    "                    {input}\"\"\"),\n",
    "                MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "            ])\n",
    "\n",
    "function_agent = create_openai_functions_agent(llm, [knowledge_graph], prompt)\n",
    "chain = AgentExecutor(agent=function_agent, tools=[knowledge_graph], verbose=True, callbacks=[ConsoleCallbackHandler()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import map_to_base_node, map_to_base_relationship\n",
    "from langchain_community.graphs.graph_document import GraphDocument\n",
    "\n",
    "# Convert our simplified graph to a Langchain graph document\n",
    "\n",
    "\n",
    "def extract_and_store_graph(data: KnowledgeGraph, document: Document) -> Optional[GraphDocument]:\n",
    "    # Extract graph data using OpenAI functions\n",
    "            \n",
    "        # Construct a graph document\n",
    "        nodes = []\n",
    "        rels = []\n",
    "        try:\n",
    "            nodes= list(map(map_to_base_node, data.nodes))\n",
    "            rels= map_to_base_relationship(data.rels, nodes)\n",
    "        except Exception as e:\n",
    "            print(\"parsing exception\")\n",
    "            print(e)\n",
    "        \n",
    "        if len(nodes) == 0:\n",
    "            return None\n",
    "            \n",
    "        return GraphDocument(\n",
    "            nodes = nodes,\n",
    "            relationships = rels,\n",
    "            source = document\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "\n",
    "local_folder = \"./data/\"\n",
    "os.makedirs(local_folder,exist_ok=True)\n",
    "\n",
    "doc_names = []\n",
    "\n",
    "documents = [\n",
    "\"https://www.marinhumane.org/wp-content/uploads/2017/06/Dog-Breed-Characteristics-Behavior.pdf\" \n",
    "]\n",
    "for doc in tqdm(documents):\n",
    "    print(\"Downloading\", doc)\n",
    "    doc_names.append(doc.split(\"/\")[-1])\n",
    "    if os.path.isfile(local_folder + doc.split(\"/\")[-1]):\n",
    "        continue\n",
    "    urllib.request.urlretrieve(doc, local_folder + doc.split(\"/\")[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to Txt\n",
    "Read and chuck the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pdf import parse_pdf\n",
    "\n",
    "# Currently this utility only uses pypdf. For more serious stuff you should use Azure Document Intelligence or similar service.\n",
    "\n",
    "docs_pages_map = dict()\n",
    "for doc in doc_names:\n",
    "    print(\"Processing \",doc)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    doc_map = parse_pdf(file=local_folder+doc)\n",
    "    docs_pages_map[doc]= doc_map\n",
    "    \n",
    "    # Capture the end time and Calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Processed {len(doc_map)} pages in {elapsed_time:.6f} seconds\\n\")\n",
    "    \n",
    "print(docs_pages_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents and execute LLM\n",
    "Parse the docs with LLM to extract the graph.\n",
    "This will take some time. Later we store the graph into pickle,so that you don't need to do this all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "full_doc = \"\"\n",
    "graph_docs = []\n",
    "\n",
    "\n",
    "for doc_name,doc_map in docs_pages_map.items():    \n",
    "    for page in tqdm(doc_map):\n",
    "        try:\n",
    "            text = page[2].strip()\n",
    "            # This will update the list function_responses. It's a global variable. (Not optimal I know)\n",
    "            data = chain.invoke(\n",
    "                {\n",
    "                    \"input\": text,\n",
    "                }\n",
    "            )      \n",
    "            for graph_doc in function_responses:\n",
    "                tmp = extract_and_store_graph(graph_doc, Document(page_content=text))\n",
    "                if tmp:                                    \n",
    "                    graph_docs.append(tmp)\n",
    "                \n",
    "            function_responses.clear()\n",
    "        except OutputParserException as e:\n",
    "            print(\"output exception\")\n",
    "            print(e)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pickle the graph so we don't have to redo this all the time\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('./data/graph_docs.pkl','wb') as f:\n",
    "    pickle.dump(graph_docs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
